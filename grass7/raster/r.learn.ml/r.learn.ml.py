#!/usr/bin/env python
############################################################################
# MODULE:        r.learn.ml
# AUTHOR:        Steven Pawley
# PURPOSE:       Supervised classification and regression of GRASS rasters
#                using the python scikit-learn package
#
# COPYRIGHT: (c) 2016 Steven Pawley, and the GRASS Development Team
#                This program is free software under the GNU General Public
#                for details.
#
#############################################################################

#%module
#% description: Supervised classification and regression of GRASS rasters using the python scikit-learn package
#% keyword: classification
#% keyword: regression
#% keyword: machine learning
#% keyword: scikit-learn
#%end

#%option G_OPT_I_GROUP
#% key: group
#% label: Imagery group to be classified
#% description: Series of raster maps to be used in the random forest classification
#% required: yes
#% multiple: no
#%end

#%option G_OPT_R_INPUT
#% key: trainingmap
#% label: Labelled pixels
#% description: Raster map with labelled pixels for training
#% required: no
#% guisection: Required
#%end

#%option G_OPT_R_OUTPUT
#% key: output
#% required: yes
#% label: Output Map
#% description: Prediction surface result from classification or regression model
#%end

#%option string
#% key: classifier
#% required: yes
#% label: Classifier
#% description: Supervised learning model to use
#% answer: RandomForestClassifier
#% options: LogisticRegression,LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis,GaussianNB,DecisionTreeClassifier,DecisionTreeRegressor,RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor,SVC,EarthClassifier,EarthRegressor,XGBClassifier,XGBRegressor
#%end

#%option
#% key: c
#% type: double
#% description: Inverse of regularization strength (logistic regresson and SVC)
#% answer: 1.0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: max_features
#% type: integer
#% description: Number of features to consider during splitting for tree based classifiers. Default is sqrt(n_features) for classification, and n_features for regression
#% required: no
#% answer:
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: max_depth
#% type: integer
#% description: Optionally specifiy maximum tree depth. Otherwise full-growing occurs for decision trees and random forests, and max_depth=3 for gradient boosting
#% required: no
#% answer:
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: min_samples_split
#% type: double
#% description: The minimum number of samples required for node splitting in tree based classifiers
#% answer: 2
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: min_samples_leaf
#% type: integer
#% description: The minimum number of samples required to form a leaf node for tree based classifiers
#% answer: 1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: n_estimators
#% type: integer
#% description: Number of estimators for tree-based classifiers
#% answer: 100
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: learning_rate
#% type: double
#% description: learning rate for gradient boosting
#% answer: 0.1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: subsample
#% type: double
#% description: The fraction of samples to be used for fitting for gradient boosting
#% answer: 1.0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option integer
#% key: max_degree
#% description: The maximum degree of terms generated by the forward pass in Earth
#% answer: 1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

# General options

#%flag
#% key: s
#% label: Standardization preprocessing
#% guisection: Optional
#%end

#%flag
#% key: i
#% label: Impute missing values in training data
#% guisection: Optional
#%end

#%option string
#% key: categorymaps
#% required: no
#% multiple: yes
#% label: Indices of categorical rasters within the imagery group (0..n)
#% description: Indices of categorical rasters within the imagery group (0..n)
#%end

#%option string
#% key: cvtype
#% required: no
#% label: Non-spatial or spatial cross-validation
#% description: Non-spatial, clumped or clustered k-fold cross-validation
#% answer: Non-spatial
#% options: non-spatial,clumped,kmeans
#%end

#%option
#% key: n_partitions
#% type: integer
#% description: Number of kmeans spatial partitions
#% answer: 10
#% guisection: Optional
#%end

#%option G_OPT_R_INPUT
#% key: group_raster
#% label: Custom group ids for labelled pixels from GRASS raster
#% description: GRASS raster containing group ids for labelled pixels
#% required: no
#% guisection: Optional
#%end

#%option
#% key: cv
#% type: integer
#% description: Number of cross-validation folds for performance evaluation
#% answer: 1
#% guisection: Optional
#%end

#%option
#% key: random_state
#% type: integer
#% description: Seed to pass onto the random state for reproducible results
#% answer: 1
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: errors_file
#% label: Save cross-validation global accuracy results to csv
#% required: no
#% guisection: Optional
#%end

#%option
#% key: lines
#% type: integer
#% description: Processing block size in terms of number of rows
#% answer: 25
#% guisection: Optional
#%end

#%flag
#% key: p
#% label: Output class membership probabilities
#% guisection: Optional
#%end

#%flag
#% key: m
#% description: Build model only - do not perform prediction
#% guisection: Optional
#%end

#%flag
#% key: f
#% description: Calculate feature importances using permutation
#% guisection: Optional
#%end

#%option
#% key: tune_cv
#% type: integer
#% description: Number of cross-validation folds used for parameter tuning
#% answer: 3
#% guisection: Optional
#%end

#%option
#% key: n_permutations
#% type: integer
#% description: Number of permutations to perform for feature importances
#% answer: 10
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: fimp_file
#% label: Save feature importances to csv
#% required: no
#% guisection: Optional
#%end

#%flag
#% key: b
#% description: Balance training data by random oversampling
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: save_training
#% label: Save training data to csv
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_INPUT
#% key: load_training
#% label: Load training data from csv
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: save_model
#% label: Save model from file
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_INPUT
#% key: load_model
#% label: Load model from file
#% required: no
#% guisection: Optional
#%end

#%flag
#% key: l
#% label: Use memory swap
#% guisection: Optional
#%end

#%rules
#% exclusive: trainingmap,load_model
#% exclusive: load_training,save_training
#% exclusive: trainingmap,load_training
#%end

import atexit
import numpy as np
import grass.script as grass
from copy import deepcopy
from grass.pygrass.modules.shortcuts import raster as r
from grass.pygrass.utils import set_path

set_path('r.learn.ml')
from raster_learning import train, model_classifiers
from raster_learning import save_training_data, load_training_data
from raster_learning import extract, maps_from_group


def cleanup():
    grass.run_command("g.remove", name='tmp_clfmask',
                      flags="f", type="raster", quiet=True)
    grass.run_command("g.remove", name='tmp_roi_clumped',
                      flags="f", type="raster", quiet=True)


def main():

    try:
        from sklearn.externals import joblib
        from sklearn.cluster import KMeans
        from sklearn.metrics import make_scorer, cohen_kappa_score
    except:
        grass.fatal("Scikit learn 0.18 or newer is not installed")

    """
    GRASS options and flags
    -----------------------
    """

    # General options and flags
    group = options['group']
    response = options['trainingmap']
    output = options['output']
    classifier = options['classifier']
    norm_data = flags['s']
    cv = int(options['cv'])
    cvtype = options['cvtype']
    group_raster = options['group_raster']
    categorymaps = options['categorymaps']
    n_partitions = int(options['n_partitions'])
    modelonly = flags['m']
    probability = flags['p']
    rowincr = int(options['lines'])
    random_state = int(options['random_state'])
    model_save = options['save_model']
    model_load = options['load_model']
    load_training = options['load_training']
    save_training = options['save_training']
    importances = flags['f']
    tune_cv = int(options['tune_cv'])
    n_permutations = int(options['n_permutations'])
    lowmem = flags['l']
    impute = flags['i']
    errors_file = options['errors_file']
    fimp_file = options['fimp_file']
    balance = flags['b']

    if ',' in categorymaps:
        categorymaps = [int(i) for i in categorymaps.split(',')]
    else:
        categorymaps = None

    param_grid = {'C': None,
                  'min_samples_split': None,
                  'min_samples_leaf': None,
                  'n_estimators': None,
                  'learning_rate': None,
                  'subsample': None,
                  'max_depth': None,
                  'max_features': None,
                  'max_degree': None}

    # classifier options
    C = options['c']
    if ',' in C:
        param_grid['C'] = [float(i) for i in C.split(',')]
        C = None
    else:
        C = float(C)

    min_samples_split = options['min_samples_split']
    if ',' in min_samples_split:
        param_grid['min_samples_split'] = \
            [float(i) for i in min_samples_split.split(',')]
        min_samples_split = None
    else:
        min_samples_split = int(min_samples_split)

    min_samples_leaf = options['min_samples_leaf']
    if ',' in min_samples_leaf:
        param_grid['min_samples_leaf'] = \
            [int(i) for i in min_samples_leaf.split(',')]
        min_samples_leaf = None
    else:
        min_samples_leaf = int(min_samples_leaf)

    n_estimators = options['n_estimators']
    if ',' in n_estimators:
        param_grid['n_estimators'] = [int(i) for i in n_estimators.split(',')]
        n_estimators = None
    else:
        n_estimators = int(n_estimators)

    learning_rate = options['learning_rate']
    if ',' in learning_rate:
        param_grid['learning_rate'] = \
            [float(i) for i in learning_rate.split(',')]
        learning_rate = None
    else:
        learning_rate = float(learning_rate)

    subsample = options['subsample']
    if ',' in subsample:
        param_grid['subsample'] = [float(i) for i in subsample.split(',')]
        subsample = None
    else:
        subsample = float(subsample)

    max_depth = options['max_depth']
    if max_depth == '':
        max_depth = None
    else:
        if ',' in max_depth:
            param_grid['max_depth'] = [int(i) for i in max_depth.split(',')]
            max_depth = None
        else:
            max_depth = int(max_depth)

    max_features = options['max_features']
    if max_features == '':
        max_features = 'auto'
    else:
        if ',' in max_features:
            param_grid['max_features'] = \
                [int(i) for i in max_features.split(',')]
            max_features = None
        else:
            max_features = int(max_features)

    max_degree = options['max_degree']
    if ',' in max_degree:
        param_grid['max_degree'] = [int(i) for i in max_degree.split(',')]
        max_degree = None
    else:
        max_degree = int(max_degree)

    if importances is True and cv == 1:
        grass.fatal('Feature importances require cross-validation cv > 1')

    # fetch individual raster names from group
    maplist, map_names = maps_from_group(group)

    """
    Sample training data and group ids
    --------------------
    """

    if model_load == '':

        # Sample training data and group id
        if load_training != '':
            X, y, group_id = load_training_data(load_training)
        else:
            # clump the labelled pixel raster if labels represent polygons
            # then set the group_raster to the clumped raster to extract the
            # group_ids used in the GroupKFold cross-validation
            if cvtype == 'clumped' and group_raster == '':
                r.clump(input=response, output='tmp_roi_clumped',
                        overwrite=True, quiet=True)
                group_raster = 'tmp_roi_clumped'

            # extract training data from maplist and take group ids from
            # group_raster. Shuffle=False so that group ids and labels align
            # because cross-validation will be performed spatially
            if group_raster != '':
                maplist2 = deepcopy(maplist)
                maplist2.append(group_raster)
                X, y, sample_coords = extract(
                        response=response, predictors=maplist2,
                        impute=impute, shuffle_data=False, lowmem=False,
                        random_state=random_state)

                # take group id from last column and remove from predictors
                group_id = X[:, -1]
                X = np.delete(X, -1, axis=1)

                # remove the clumped raster
                try:
                    grass.run_command(
                        "g.remove", name='tmp_roi_clumped', flags="f",
                        type="raster", quiet=True)
                except:
                    pass

            else:
                # extract training data from maplist without group Ids
                # shuffle this data by default
                X, y, sample_coords = extract(
                    response=response, predictors=maplist,
                    impute=impute,
                    shuffle_data=True,
                    lowmem=lowmem,
                    random_state=random_state)

                group_id = None

                if cvtype == 'kmeans':
                    clusters = KMeans(n_clusters=n_partitions,
                                      random_state=random_state,
                                      n_jobs=-1)

                    clusters.fit(sample_coords)
                    group_id = clusters.labels_

            # check for labelled pixels and training data
            if y.shape[0] == 0 or X.shape[0] == 0:
                grass.fatal('No training pixels or pixels in imagery group '
                            '...check computational region')

        # option to save extracted data to .csv file
        if save_training != '':
            save_training_data(X, y, group_id, save_training)

        """
        Train the classifier
        --------------------
        """

        # retrieve sklearn classifier object and parameters
        grass.message("Classifier = " + classifier)

        clf, mode = \
            model_classifiers(classifier, random_state,
                              C, max_depth, max_features, min_samples_split,
                              min_samples_leaf, n_estimators,
                              subsample, learning_rate, max_degree)

        # turn off balancing if mode = regression
        if mode == 'regression' and balance is True:
            balance = False

        # remove empty items from the param_grid dict
        param_grid = {k: v for k, v in param_grid.iteritems() if v is not None}

        # check that dict keys are compatible for the selected classifier
        clf_params = clf.get_params()
        param_grid = { key: value for key, value in param_grid.iteritems() if key in clf_params}

        # check if dict contains and keys, otherwise set it to None
        # so that the train object will not perform GridSearchCV
        if any(param_grid) is not True:
            param_grid = None

        # Decide on scoring metric scheme and scorer to for grid search
        if mode == 'classification':
            if len(np.unique(y)) == 2 and all([0, 1] == np.unique(y)):
                scorers = 'binary'
            else:
                scorers = 'multiclass'
            search_scorer = make_scorer(cohen_kappa_score)
        else:
            scorers = 'regression'
            search_scorer = 'r2'

        if mode == 'regression' and probability is True:
            grass.warning(
                'Class probabilities only valid for classifications...'
                'ignoring')
            probability = False

        # create training object - onehot-encoded on-the-fly
        learn_m = train(clf, X, y, group_id, categorical_var=categorymaps,
                        standardize=norm_data, balance=balance)

        """
        Fitting, parameter search and cross-validation
        ----------------
        """

        # fit and parameter search
        learn_m.fit(param_grid=param_grid, cv=tune_cv, scoring=search_scorer,
                    random_state=random_state)

        if param_grid is not None:
            grass.message('\n')
            grass.message('Best parameters:')
            grass.message(str(learn_m.estimator.best_params_))

        # If cv > 1 then use cross-validation to generate performance measures
        if cv > 1:
            grass.message('\r\n')
            grass.message(
                "Cross validation global performance measures......:")

            # cross-validate the training object
            learn_m.cross_val(scorers, cv, importances,
                              n_permutations=n_permutations,
                              random_state=random_state)

            if mode == 'classification':
                if scorers == 'binary':
                    grass.message(
                        "Accuracy   :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['accuracy'].mean(),
                         learn_m.scores['accuracy'].std()))
                    grass.message(
                        "AUC        :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['auc'].mean(),
                         learn_m.scores['auc'].std()))
                    grass.message(
                        "Kappa      :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['kappa'].mean(),
                         learn_m.scores['kappa'].std()))
                    grass.message(
                        "Precision  :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['precision'].mean(),
                         learn_m.scores['precision'].std()))
                    grass.message(
                        "Recall     :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['recall'].mean(),
                         learn_m.scores['recall'].std()))
                    grass.message(
                        "Specificity:\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['specificity'].mean(),
                         learn_m.scores['specificity'].std()))
                    grass.message(
                        "F1         :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['f1'].mean(),
                         learn_m.scores['f1'].std()))

                if scorers == 'multiclass':
                    grass.message(
                        "Accuracy:\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['accuracy'].mean(),
                         learn_m.scores['accuracy'].std()))
                    grass.message(
                        "Kappa   :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['kappa'].mean(),
                         learn_m.scores['kappa'].std()))

                # classification report
                grass.message("\n")
                grass.message("Classification report:")
                grass.message(learn_m.scores_cm)

            else:
                grass.message("R2:\t%0.3f\t+/-\t%0.3f" %
                              (learn_m.scores['r2'].mean(),
                               learn_m.scores['r2'].std()))

            # write cross-validation results for csv file
            if errors_file != '':
                try:
                    import pandas as pd
                    errors = pd.DataFrame(learn_m.scores)
                    errors.to_csv(errors_file, mode='w')
                except:
                    grass.warning('Pandas is not installed. Pandas is '
                                  'required to write the cross-validation '
                                  'results to file')

            # feature importances
            if importances is True:
                grass.message("\r\n")
                grass.message("Feature importances")
                grass.message("id" + "\t" + "Raster" + "\t" + "Importance")

                # mean of cross-validation feature importances
                for i in range(len(learn_m.fimp.mean(axis=0))):
                    grass.message(
                        str(i) + "\t" + maplist[i] +
                        "\t" + str(round(learn_m.fimp.mean(axis=0)[i], 4)))

                if fimp_file != '':
                    np.savetxt(fname=fimp_file, X=learn_m.fimp, delimiter=',',
                               header=','.join(maplist), comments='')
    else:
        # load a previously fitted train object
        # -------------------------------------
        if model_load != '':
            # load a previously fitted model
            learn_m = joblib.load(model_load)

    """
    Optionally save the fitted model
    ---------------------
    """

    if model_save != '':
        joblib.dump(learn_m, model_save)

    """
    Prediction on the rest of the GRASS rasters in the imagery group
    ----------------------------------------------------------------
    """
    if modelonly is not True:
        learn_m.predict(maplist, output, probability, rowincr)
    else:
        grass.message("Model built and now exiting")

if __name__ == "__main__":
    options, flags = grass.parser()
    atexit.register(cleanup)
    main()
