#!/usr/bin/env python
############################################################################
# MODULE:        r.learn.ml
# AUTHOR:        Steven Pawley
# PURPOSE:       Supervised classification and regression of GRASS rasters
#                using the python scikit-learn package
#
# COPYRIGHT: (c) 2016 Steven Pawley, and the GRASS Development Team
#                This program is free software under the GNU General Public
#                for details.
#
#############################################################################

#%module
#% description: Supervised classification and regression of GRASS rasters using the python scikit-learn package
#% keyword: classification
#% keyword: regression
#% keyword: machine learning
#% keyword: scikit-learn
#%end

#%option G_OPT_I_GROUP
#% key: group
#% label: Imagery group to be classified
#% description: Series of raster maps to be used in the random forest classification
#% required: yes
#% multiple: no
#%end

#%option G_OPT_R_INPUT
#% key: trainingmap
#% label: Labelled pixels
#% description: Raster map with labelled pixels for training
#% required: no
#% guisection: Required
#%end

#%option G_OPT_R_OUTPUT
#% key: output
#% required: yes
#% label: Output Map
#% description: Prediction surface result from classification or regression model
#%end

#%option string
#% key: classifier
#% required: yes
#% label: Classifier
#% description: Supervised learning model to use
#% answer: RandomForestClassifier
#% options: LogisticRegression,LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis,GaussianNB,DecisionTreeClassifier,DecisionTreeRegressor,RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor,SVC,EarthClassifier,EarthRegressor,XGBClassifier,XGBRegressor
#%end

#%option
#% key: c
#% type: double
#% description: Inverse of regularization strength (logistic regresson and SVC)
#% answer: 1.0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: max_features
#% type: integer
#% description: Number of features to consider during splitting for tree based classifiers. Default is sqrt(n_features) for classification, and n_features for regression
#% required: no
#% answer:0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: max_depth
#% type: integer
#% description: Optionally specifiy maximum tree depth. Otherwise full-growing occurs for decision trees and random forests, and max_depth=3 for gradient boosting
#% required: no
#% answer:0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: min_samples_split
#% type: double
#% description: The minimum number of samples required for node splitting in tree based classifiers
#% answer: 2
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: min_samples_leaf
#% type: integer
#% description: The minimum number of samples required to form a leaf node for tree based classifiers
#% answer: 1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: n_estimators
#% type: integer
#% description: Number of estimators for tree-based classifiers
#% answer: 100
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: learning_rate
#% type: double
#% description: learning rate for gradient boosting
#% answer: 0.1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: subsample
#% type: double
#% description: The fraction of samples to be used for fitting for gradient boosting
#% answer: 1.0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option integer
#% key: max_degree
#% description: The maximum degree of terms generated by the forward pass in Earth
#% answer: 1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

# General options

#%flag
#% key: s
#% label: Standardization preprocessing
#% guisection: Optional
#%end

#%flag
#% key: i
#% label: Impute missing values in training data
#% guisection: Optional
#%end

#%option string
#% key: categorymaps
#% required: no
#% multiple: yes
#% label: Indices of categorical rasters within the imagery group (0..n)
#% description: Indices of categorical rasters within the imagery group (0..n)
#%end

#%option string
#% key: cvtype
#% required: no
#% label: Non-spatial or spatial cross-validation
#% description: Non-spatial, clumped or clustered k-fold cross-validation
#% answer: Non-spatial
#% options: non-spatial,clumped,kmeans
#%end

#%option
#% key: n_partitions
#% type: integer
#% description: Number of kmeans spatial partitions
#% answer: 10
#% guisection: Optional
#%end

#%option G_OPT_R_INPUT
#% key: group_raster
#% label: Custom group ids for labelled pixels from GRASS raster
#% description: GRASS raster containing group ids for labelled pixels
#% required: no
#% guisection: Optional
#%end

#%option
#% key: cv
#% type: integer
#% description: Number of cross-validation folds for performance evaluation
#% answer: 1
#% guisection: Optional
#%end

#%option
#% key: random_state
#% type: integer
#% description: Seed to pass onto the random state for reproducible results
#% answer: 1
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: errors_file
#% label: Save cross-validation global accuracy results to csv
#% required: no
#% guisection: Optional
#%end

#%option
#% key: lines
#% type: integer
#% description: Processing block size in terms of number of rows
#% answer: 25
#% guisection: Optional
#%end

#%flag
#% key: p
#% label: Output class membership probabilities
#% guisection: Optional
#%end

#%flag
#% key: m
#% description: Build model only - do not perform prediction
#% guisection: Optional
#%end

#%flag
#% key: f
#% description: Calculate feature importances using permutation
#% guisection: Optional
#%end

#%option
#% key: n_permutations
#% type: integer
#% description: Number of permutations to perform for feature importances
#% answer: 10
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: fimp_file
#% label: Save feature importances to csv
#% required: no
#% guisection: Optional
#%end

#%flag
#% key: b
#% description: Balance training data by random oversampling
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: save_training
#% label: Save training data to csv
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_INPUT
#% key: load_training
#% label: Load training data from csv
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: save_model
#% label: Save model from file
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_INPUT
#% key: load_model
#% label: Load model from file
#% required: no
#% guisection: Optional
#%end

#%flag
#% key: l
#% label: Use memory swap
#% guisection: Optional
#%end

#%rules
#% exclusive: trainingmap,load_model
#% exclusive: load_training,save_training
#% exclusive: trainingmap,load_training
#%end

import atexit
import os
import numpy as np
import grass.script as grass
from copy import deepcopy
from grass.pygrass.modules.shortcuts import raster as r
from grass.pygrass.utils import set_path

set_path('r.learn.ml')
from raster_learning import train, model_classifiers
from raster_learning import save_training_data, load_training_data
from raster_learning import extract, maps_from_group, random_oversampling

tmp_rast = []


def cleanup():
    for rast in tmp_rast:
        grass.run_command("g.remove", rast=rast, quiet=True)


def main():

    """
    Lazy imports for main------------------------------------------------------
    """

    try:
        from sklearn.externals import joblib
        from sklearn.cluster import KMeans
        from sklearn.metrics import make_scorer, cohen_kappa_score
        from sklearn.model_selection import StratifiedKFold, GroupKFold
        from sklearn.preprocessing import StandardScaler
        from sklearn.model_selection import GridSearchCV
        import warnings
        warnings.filterwarnings('ignore')  # turn off UndefinedMetricWarning
    except:
        grass.fatal("Scikit learn 0.18 or newer is not installed")

    """
    GRASS options and flags----------------------------------------------------
    """

    # General options and flags -----------------------------------------------
    group = options['group']
    response = options['trainingmap']
    output = options['output']
    classifier = options['classifier']
    norm_data = flags['s']
    cv = int(options['cv'])
    cvtype = options['cvtype']
    group_raster = options['group_raster']
    categorymaps = options['categorymaps']
    n_partitions = int(options['n_partitions'])
    modelonly = flags['m']
    probability = flags['p']
    rowincr = int(options['lines'])
    random_state = int(options['random_state'])
    model_save = options['save_model']
    model_load = options['load_model']
    load_training = options['load_training']
    save_training = options['save_training']
    importances = flags['f']
    n_permutations = int(options['n_permutations'])
    lowmem = flags['l']
    impute = flags['i']
    errors_file = options['errors_file']
    fimp_file = options['fimp_file']
    balance = flags['b']

    if ',' in categorymaps:
        categorymaps = [int(i) for i in categorymaps.split(',')]
    else:
        categorymaps = None

    # classifier options and parameter grid settings --------------------------
    param_grid = {'C': None,
                  'min_samples_split': None,
                  'min_samples_leaf': None,
                  'n_estimators': None,
                  'learning_rate': None,
                  'subsample': None,
                  'max_depth': None,
                  'max_features': None,
                  'max_degree': None}

    C = options['c']
    if ',' in C:
        param_grid['C'] = [float(i) for i in C.split(',')]
        C = None
    else:
        C = float(C)

    min_samples_split = options['min_samples_split']
    if ',' in min_samples_split:
        param_grid['min_samples_split'] = \
            [float(i) for i in min_samples_split.split(',')]
        min_samples_split = None
    else:
        min_samples_split = int(min_samples_split)

    min_samples_leaf = options['min_samples_leaf']
    if ',' in min_samples_leaf:
        param_grid['min_samples_leaf'] = \
            [int(i) for i in min_samples_leaf.split(',')]
        min_samples_leaf = None
    else:
        min_samples_leaf = int(min_samples_leaf)

    n_estimators = options['n_estimators']
    if ',' in n_estimators:
        param_grid['n_estimators'] = [int(i) for i in n_estimators.split(',')]
        n_estimators = None
    else:
        n_estimators = int(n_estimators)

    learning_rate = options['learning_rate']
    if ',' in learning_rate:
        param_grid['learning_rate'] = \
            [float(i) for i in learning_rate.split(',')]
        learning_rate = None
    else:
        learning_rate = float(learning_rate)

    subsample = options['subsample']
    if ',' in subsample:
        param_grid['subsample'] = [float(i) for i in subsample.split(',')]
        subsample = None
    else:
        subsample = float(subsample)

    max_depth = options['max_depth']
    if max_depth == '0':
        max_depth = None
    else:
        if ',' in max_depth:
            param_grid['max_depth'] = [int(i) for i in max_depth.split(',')]
            max_depth = None
        else:
            max_depth = int(max_depth)

    max_features = options['max_features']
    if max_features == '0':
        max_features = 'auto'
    else:
        if ',' in max_features:
            param_grid['max_features'] = \
                [int(i) for i in max_features.split(',')]
            max_features = None
        else:
            max_features = int(max_features)

    max_degree = options['max_degree']
    if ',' in max_degree:
        param_grid['max_degree'] = [int(i) for i in max_degree.split(',')]
        max_degree = None
    else:
        max_degree = int(max_degree)

    # remove empty items from the param_grid dict
    param_grid = {k: v for k, v in param_grid.iteritems() if v is not None}

    if importances is True and cv == 1:
        grass.fatal('Feature importances require cross-validation cv > 1')

    # fetch individual raster names from group --------------------------------
    maplist, map_names = maps_from_group(group)

    """
    Sample training data and group ids ----------------------------------------
    """

    if model_load == '':

        # Sample training data and group id
        if load_training != '':
            X, y, group_id = load_training_data(load_training)
        else:
            grass.message('Extracting training data')

            # clump the labelled pixel raster if labels represent polygons
            # then set the group_raster to the clumped raster to extract the
            # group_ids used in the GroupKFold cross-validation
            if cvtype == 'clumped' and group_raster == '':
                clumped_trainingmap = 'tmp_clumped_trainingmap'
                tmp_rast.append(clumped_trainingmap)
                r.clump(input=response, output=clumped_trainingmap,
                        overwrite=True, quiet=True)
                group_raster = clumped_trainingmap

            # extract training data from maplist and take group ids from
            # group_raster. Shuffle=False so that group ids and labels align
            # because cross-validation will be performed spatially
            if group_raster != '':
                maplist2 = deepcopy(maplist)
                maplist2.append(group_raster)
                X, y, sample_coords = extract(
                        response=response, predictors=maplist2,
                        impute=impute, shuffle_data=False, lowmem=False,
                        random_state=random_state)

                # take group id from last column and remove from predictors
                group_id = X[:, -1]
                X = np.delete(X, -1, axis=1)
            else:
                # extract training data from maplist without group Ids
                # shuffle this data by default
                X, y, sample_coords = extract(
                    response=response, predictors=maplist,
                    impute=impute,
                    shuffle_data=True,
                    lowmem=lowmem,
                    random_state=random_state)

                group_id = None

                if cvtype == 'kmeans':
                    clusters = KMeans(n_clusters=n_partitions,
                                      random_state=random_state,
                                      n_jobs=-1)

                    clusters.fit(sample_coords)
                    group_id = clusters.labels_

            # check for labelled pixels and training data
            if y.shape[0] == 0 or X.shape[0] == 0:
                grass.fatal('No training pixels or pixels in imagery group '
                            '...check computational region')

        # option to save extracted data to .csv file
        if save_training != '':
            save_training_data(X, y, group_id, save_training)

        """
        Train the classifier --------------------------------------------------
        """

        # retrieve sklearn classifier object and parameters -------------------
        clf, mode = \
            model_classifiers(classifier, random_state,
                              C, max_depth, max_features, min_samples_split,
                              min_samples_leaf, n_estimators,
                              subsample, learning_rate, max_degree)

        # set other parameters based on classification or regression ----------
        if mode == 'classification':
            if len(np.unique(y)) == 2 and all([0, 1] == np.unique(y)):
                scorers = 'binary'
            else:
                scorers = 'multiclass'
            search_scorer = make_scorer(cohen_kappa_score)
            labels = np.unique(y)
        else:
            scorers = 'regression'
            search_scorer = 'r2'
            labels = None  # no classes
            balance = False  # no balancing for regression
            if probability is True:
                grass.warning(
                        'Class probabilities only valid for classifications...'
                        'ignoring')
                probability = False

        # setup model selection model -----------------------------------------
        if any(param_grid) is True and cv == 1:
            grass.fatal('Hyperparameter search requires cv > 1')
        if any(param_grid) is True or cv > 1:
            if group_id is None:
                search_cv_method = StratifiedKFold(
                        n_splits=cv, random_state=random_state)
            else:
                search_cv_method = GroupKFold(n_splits=cv)

        # set-up parameter grid for hyperparameter search ---------------------
        # check that dict keys are compatible for the selected classifier
        clf_params = clf.get_params()
        param_grid = { key: value for key, value in param_grid.iteritems() if key in clf_params}

        # check if dict contains and keys, otherwise set it to None
        # so that the train object will not perform GridSearchCV
        if any(param_grid) is not True:
            param_grid = None
        else:
            clf = GridSearchCV(estimator=clf, param_grid=param_grid,
                               scoring=search_scorer, n_jobs=-1,
                               cv=search_cv_method)

        # preprocessing options -----------------------------------------------
        if balance is True:
            sampling = random_oversampling(random_state)
        else:
            sampling = None

        if norm_data is True:
            scaler = StandardScaler()
        else:
            scaler = None

        # create training object ----------------------------------------------
        learn_m = train(clf, categorical_var=categorymaps,
                        preprocessing=scaler, sampling=sampling)

        """
        Fitting, parameter search and cross-validation ------------------------
        """

        # fit and parameter search
        grass.message(os.linesep)
        grass.message(('Fitting model using ' + classifier))
        learn_m.fit(X, y, group_id)

        if param_grid is not None:
            grass.message(os.linesep)
            grass.message('Best parameters:')
            grass.message(str(learn_m.estimator.best_params_))
            
        # If cv > 1 then use cross-validation to generate performance measures
        if cv > 1:
            # check that a sufficient number of samples are present per class
            if cv > np.histogram(y, bins=len(labels))[0].min():
                grass.message(os.linesep)
                grass.message('Number of cv folds is greater than number of '
                              'samples in some classes. Cross-validation is being'
                              ' skipped')
            else:
                grass.message(os.linesep)
                grass.message(
                    "Cross validation global performance measures......:")

                # cross-validate the training object
                learn_m.cross_val(search_cv_method, X, y, group_id,
                                  scorers, importances,
                                  n_permutations=n_permutations,
                                  random_state=random_state)

                scores = learn_m.get_cross_val_scores()

                if mode == 'classification':
                    if scorers == 'binary':
                        grass.message(
                            "Accuracy   :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['accuracy'].mean(),
                             scores['accuracy'].std()))
                        grass.message(
                            "AUC        :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['auc'].mean(),
                             scores['auc'].std()))
                        grass.message(
                            "Kappa      :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['kappa'].mean(),
                             scores['kappa'].std()))
                        grass.message(
                            "Precision  :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['precision'].mean(),
                             scores['precision'].std()))
                        grass.message(
                            "Recall     :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['recall'].mean(),
                             scores['recall'].std()))
                        grass.message(
                            "Specificity:\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['specificity'].mean(),
                             scores['specificity'].std()))
                        grass.message(
                            "F1         :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['f1'].mean(),
                             scores['f1'].std()))

                    if scorers == 'multiclass':
                        # global scores
                        grass.message(
                            "Accuracy:\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['accuracy'].mean(),
                             scores['accuracy'].std()))
                        grass.message(
                            "Kappa   :\t%0.3f\t+/-SD\t%0.3f" %
                            (scores['kappa'].mean(),
                             scores['kappa'].std()))

                        # per class scores
                        grass.message(os.linesep)
                        grass.message('Cross validation class performance measures......:')
                        mat_precision = np.matrix(scores['precision'])
                        mat_recall = np.matrix(scores['recall'])
                        mat_f1 = np.matrix(scores['f1'])

                        grass.message('Class \t' + '\t'.join(map(str, labels)))
                        grass.message(
                            'Precision mean \t' + '\t'.join(
                                    map(str, np.round(
                                            mat_precision.mean(axis=0), 2)[0])))
                        grass.message(
                            'Precision std \t' + '\t'.join(
                                    map(str, np.round(
                                            mat_precision.std(axis=0), 2)[0])))
                        grass.message(
                            'Recall mean \t' + '\t'.join(
                                    map(str, np.round(
                                            mat_recall.mean(axis=0), 2)[0])))
                        grass.message(
                            'Recall std \t' + '\t'.join(
                                    map(str, np.round(
                                            mat_recall.std(axis=0), 2)[0])))
                        grass.message(
                            'F1 score mean \t' + '\t'.join(
                                    map(str, np.round(
                                            mat_f1.mean(axis=0), 2)[0])))
                        grass.message(
                            'F1 score std \t' + '\t'.join(
                                    map(str, np.round(
                                            mat_f1.std(axis=0), 2)[0])))

                        # remove perclass scores from dict
                        del scores['precision']
                        del scores['recall']
                        del scores['f1']

                else:
                    grass.message("R2:\t%0.3f\t+/-\t%0.3f" %
                                  (scores['r2'].mean(),
                                   scores['r2'].std()))

                # write cross-validation results for csv file
                if errors_file != '':
                    try:
                        import pandas as pd
                        errors = pd.DataFrame(scores)
                        errors.to_csv(errors_file, mode='w')
                    except:
                        grass.warning('Pandas is not installed. Pandas is '
                                      'required to write the cross-validation '
                                      'results to file')

                # feature importances
                if importances is True:
                    grass.message(os.linesep)
                    grass.message("Feature importances")
                    grass.message("id" + "\t" + "Raster" + "\t" + "Importance")

                    # mean of cross-validation feature importances
                    for i in range(len(learn_m.fimp.mean(axis=0))):
                        grass.message(
                            str(i) + "\t" + maplist[i] +
                            "\t" + str(round(learn_m.fimp.mean(axis=0)[i], 4)))

                    if fimp_file != '':
                        np.savetxt(fname=fimp_file, X=learn_m.fimp, delimiter=',',
                                   header=','.join(maplist), comments='')
    else:
        # load a previously fitted train object
        # -------------------------------------
        if model_load != '':
            # load a previously fitted model
            learn_m = joblib.load(model_load)

    # Optionally save the fitted model
    if model_save != '':
        joblib.dump(learn_m, model_save)

    """
    Prediction on the rest of the GRASS rasters in the imagery group ----------
    """

    if modelonly is not True:
        grass.message(os.linesep)
        grass.message('Predicting raster...')
        learn_m.predict(maplist, output, labels, probability, rowincr)
    else:
        grass.message("Model built and now exiting")

if __name__ == "__main__":
    options, flags = grass.parser()
    atexit.register(cleanup)
    main()
