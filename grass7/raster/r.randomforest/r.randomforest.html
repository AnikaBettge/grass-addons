<h2>DESCRIPTION</h2>

<em><b>r.randomforest</b></em> performs Random forests classification and regression on a suite of predictors within a GRASS imagery group. Random forest (Breiman, 2001) is an ensemble classification tree method which constructs a forest of uncorrelated decision trees based on a random subset of predictor variables, which occurs independently at every node split in each tree. Each tree produces a prediction probability, and the final classification result is obtained by averaging of the prediction probabilities across all of the trees. The probability of membership to the individual classes (<i>class_probabilities</i> flag) can also be output using the -p flag. The scikit-learn randomforest implementation differs from the original Breiman (2001) reference which uses majority voting rather than averaging.

<br><br>Random forests offers a number of advantages over traditional statistical classifiers because it is non-parametric and can deal with non-linear relationships and  categorical data, and no rescaling is required. Random forests also require relatively few user-specified parameter choices, principally consisting of the number of trees in the forest (<i>ntrees</i>), and the number of variables that are allowed to be chosen from at each node split (<i>mfeatures</i>), which controls the degree of correlation between the trees. There is no accuracy penalty in having a large number of trees apart from increased computational time. For randomforest classification, the default <i>ntrees</i> is 500 and the default setting of <i>mfeatures</i> is equal to the square root of the number of predictors.

<br><br>Random forests includes built-in accuracy assessment and variable selection. Random forests uses bagging, where a randomly selected 66% subset of the training data are held-out 'out-of-bag' (OOB) in the construction of each tree, and then OOB data are used to evaluate the prediction accuracy. The OOB error has been shown to provide a reliable estimate of classification error. However, the <i>-cv</i> can be set to > 1 to perform k-fold cross validation as an external method of accuracy assessment, and a number of global performance metrics are used.

<br><br><b>Note:</b> the performance are applied on a pixel-basis in the module and assume that the training data are not spatially correlated. This assumption is unlikely to be true of the training data represent rasterized polygons rather than points. In this case, an independent set of polygons should be used to test the accuracy of the classification using <i>i.kappa</i>.

<br><br>The random forests scikit-learn implementation includes a measure of variable importance based on the Gini impurity criterion, which measures how each variable contributes to the homogeneity of the nodes, with important variables causing a larger decrease in the Gini coefficient in successive node splits. This variable importance allows the contributions of the individual predictors to be determined. The feature importance scores are displayed in the command output.

<br><br>Random forests does not perform well in the case of a large class imbalance. In this case, the classifier will seek to reduce the overall model error, but this will occur by predicting the majority class with a very high accuracy, but at the expense of the minority class. If you have a highly imbalanced dataset, the 'balanced' flag can be set. The scikit-learn implementation balanced mode then automatically adjust weights inversely proportional to class frequencies.

<br><br>Random forest can also be run in regression mode by setting the <i>mode</i> to the regression option. In this case, the mean square error (mse) is used to measure the quality of each decision split in the tree. Additionally, the the default <i>mfeatures</i> is equal to the number of predictors, and the coefficient of determination R^2 of the prediction is used as the performance measure. The generalization ability of the classifier can also be increased using <i>minsplit</i>, which represents the minimum number of samples required in order to split a node. The balanced and class_probabilities options are ignored for regression. 

<br><br>The module also offers the ability to save and load a random forests model. The model is saved as a list of filenames (starting with the extension .pkl which is added automatically) for each numpy array. This list can involve a large number of files, so it makes sense to save each model in a separate directory. To load the model, you need to select the .pkl file that was saved. Saving and loading a model represents a useful feature because it allows a model to be built on one imagery group (ie. set of predictor variables), and then the prediction can be performed on other imagery groups. This approach is commonly employed in species prediction modelling, or landslide susceptibility modelling, where a classification or regression model is built with one set of predictors (e.g. which include present-day climatic variables) and then predictions can be performed on other imagery groups containing forecasted climatic variables. The names of the GRASS rasters in the imagery groups do not matter because scikit learn saves the model as a series of numpy arrays. However, the new imagery group must contain the same number of rasters, and they should be in the same order as in the imagery group upon which the model was built. As an example, the new imagery group may have a raster named 'mean_precipitation_2050' which substitutes the 'mean_precipitation_2016' in the imagery group that was used to build the model. 

<h2>NOTES</h2>

<em><b>r.randomforest</b></em> uses the "scikit-learn" machine learning python package. This python package needs to be installed within your GRASS GIS Python environment for <em><b>r.randomforest</b></em> to work.
<br>
For Linux users, this package should be available through the linux package manager in most distributions (named for example "python-scikit-learn").
<br>
For MS-Windows users, the easiest way of installing the packages is by using the precompiled binaries from <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">Christoph Gohlke</a> and by using the <a href="https://grass.osgeo.org/download/software/ms-windows/">OSGeo4W</a> installation method of GRASS, where the python setuptools can also be installed. You can then use 'easy_install pip' to install the pip package manager. Then, you can download the NumPy-1.10+MKL and scikit-learn .whl files and install them using 'pip install packagename.whl'.

<p>
<em><b>r.randomforest</b></em> is designed to keep system memory requirements relatively low. For this purpose, the rasters are read from the disk row-by-row, using the RasterRow method in PyGRASS. This however does not represent an efficient volume of data to pass to the classifier, which is multithreaded by default. Therefore, groups of rows specified by the <i>lines</i> parameter are passed to the classifier, and the reclassified image is reconstructed and written row-by-row back to the disk. <i>Lines=50</i> should be reasonable for most systems with 4-8 GB of ram. However, if you have a workstation with much larger resources, then <i>lines</i> could be set to a much larger size (including to a value that is equal or greater than the number of rows in the current region setting) in which case the entire image will be loaded into memory to classification.

<br><br> The bootstrapping process involved within random forests also causes a small amount of variation in the classification results, out-of-bag error, and feature importances. To enable reproducible results, a seed is supplied to the classifier. This can be changed using the <i>randst</i> parameter.

<h2>TODO</h2>

Provide option to perform cross-validation on a polygon or region basis.
Provide option to perform spatial and non-spatial cross-validation.

<h2>EXAMPLE</h2>

Here we are going to use the GRASS GIS sample North Carolina data set as a basis to perform a landsat classification. We are going to classify a Landsat 7 scene from 2000, using training information from an older (1996) land cover dataset.

<br><br>Landsat 7 (2000) bands 7,4,2 color composite example:
<center>
<img src="lsat7_2000_b742.png" alt="Landsat 7 (2000) bands 7,4,2 color composite example">
</center>

Note that this example must be run in the "landsat" mapset of the North Carolina sample data set location.

<br><br>First, we are going to generate some training pixels from an older (1996) land cover classification:
<div class="code"><pre>
g.region raster=landclass96 -p
r.random input=landclass96 npoints=1000 raster=landclass96_roi
</pre></div>

<br><br>Then we can use these training pixels to perform a classification on the more recently obtained landsat 7 image:
<div class="code"><pre>
r.randomforest igroup=lsat7_2000 roi=landclass96_roi output=rf_classification \
  mode=classification ntrees=500 mfeatures=-1 minsplit=2 randst=1 lines=100

# copy category labels from landclass training map to result
r.category rf_classification raster=landclass96_roi

# copy color scheme from landclass training map to result
r.colors rf_classification raster=landclass96_roi
r.category rf_classification
</pre></div>

<!-- also nice
r.composite b=lsat7_2002_10 g=lsat7_2002_20 r=lsat7_2002_30 output=lsat7_2002.rgb
g.gui.mapswipe first=lsat7_2002.rgb second=rf_classification
-->

<br><br>Random forest classification result:
<center>
<img src="rfclassification.png" alt="Random forest classification result">
</center>

<h2>REFERENCES</h2>

Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32.

<h2>ACKNOWLEDGEMENTS</h2>

Thanks for Paulo van Breugel for general testing, and particularly the suggestion to enable random forest prediction of a different set of predictor variables.

<h2>AUTHOR</h2>

Steven Pawley

<p><i>Last changed: $Date$</i>
